{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30408,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing the relevant libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom glob import glob\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport cv2, warnings\nwarnings.filterwarnings('ignore')\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Flatten, Input, Add, Dropout, LSTM, TimeDistributed, Embedding, RepeatVector, Concatenate, Bidirectional, Convolution2D\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.utils import to_categorical, plot_model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:55:33.969496Z","iopub.execute_input":"2023-03-21T17:55:33.970823Z","iopub.status.idle":"2023-03-21T17:55:44.190456Z","shell.execute_reply.started":"2023-03-21T17:55:33.970771Z","shell.execute_reply":"2023-03-21T17:55:44.189068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the images","metadata":{}},{"cell_type":"code","source":"img_path = '/kaggle/input/flickr8k/Images/'\nimages = glob(img_path+'*.jpg')\nimages[:5]","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:55:44.192997Z","iopub.execute_input":"2023-03-21T17:55:44.194131Z","iopub.status.idle":"2023-03-21T17:55:44.325608Z","shell.execute_reply.started":"2023-03-21T17:55:44.194087Z","shell.execute_reply":"2023-03-21T17:55:44.324388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(images)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:55:44.327367Z","iopub.execute_input":"2023-03-21T17:55:44.328036Z","iopub.status.idle":"2023-03-21T17:55:44.338663Z","shell.execute_reply.started":"2023-03-21T17:55:44.32799Z","shell.execute_reply":"2023-03-21T17:55:44.336256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the captions","metadata":{}},{"cell_type":"code","source":"captions = open('/kaggle/input/flickr8k/captions.txt','rb').read().decode('utf-8').split('\\n')\ncaptions[:5]","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:55:44.343844Z","iopub.execute_input":"2023-03-21T17:55:44.344624Z","iopub.status.idle":"2023-03-21T17:55:44.389122Z","shell.execute_reply.started":"2023-03-21T17:55:44.344582Z","shell.execute_reply":"2023-03-21T17:55:44.38801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(captions)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:55:44.391085Z","iopub.execute_input":"2023-03-21T17:55:44.391951Z","iopub.status.idle":"2023-03-21T17:55:44.400224Z","shell.execute_reply.started":"2023-03-21T17:55:44.391908Z","shell.execute_reply":"2023-03-21T17:55:44.399189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing images along with their captions","metadata":{}},{"cell_type":"code","source":"for i in range(5):\n    plt.figure(figsize=(5,5))\n    img = cv2.imread(images[i])\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    plt.imshow(img);","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:55:44.403616Z","iopub.execute_input":"2023-03-21T17:55:44.40395Z","iopub.status.idle":"2023-03-21T17:55:46.071483Z","shell.execute_reply.started":"2023-03-21T17:55:44.403915Z","shell.execute_reply":"2023-03-21T17:55:46.070525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Downloading the ResNet50 inception model","metadata":{}},{"cell_type":"code","source":"inception_model = ResNet50(include_top=True)\ninception_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:55:46.073213Z","iopub.execute_input":"2023-03-21T17:55:46.073976Z","iopub.status.idle":"2023-03-21T17:55:52.791886Z","shell.execute_reply.started":"2023-03-21T17:55:46.073938Z","shell.execute_reply":"2023-03-21T17:55:52.791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"last = inception_model.layers[-2].output # Output of the penultimate layer of ResNet model \nmodel = Model(inputs=inception_model.input,outputs=last)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:55:52.792996Z","iopub.execute_input":"2023-03-21T17:55:52.793459Z","iopub.status.idle":"2023-03-21T17:55:53.382501Z","shell.execute_reply.started":"2023-03-21T17:55:52.79342Z","shell.execute_reply":"2023-03-21T17:55:53.381639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extracting features from images","metadata":{}},{"cell_type":"code","source":"img_features = {}\ncount = 0\n\nfor img_path in tqdm(images):\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img,(224,224)) # ResNet model requires images of dimensions (224,224,3)\n    img = img.reshape(1,224,224,3) # Reshaping image to the dimensions of a single image\n    features = model.predict(img).reshape(2048,) # Feature extraction from images\n    img_name = img_path.split('/')[-1] # Extracting image name\n    img_features[img_name] = features\n    count += 1\n    # Fetching the features of only 1500 images as using more than 1500 images leads to overloading memory issues\n    if count == 1500:\n        break\n    if count % 50 == 0:\n        print(count)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:55:53.383659Z","iopub.execute_input":"2023-03-21T17:55:53.384087Z","iopub.status.idle":"2023-03-21T17:58:21.04271Z","shell.execute_reply.started":"2023-03-21T17:55:53.384044Z","shell.execute_reply":"2023-03-21T17:58:21.04162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(img_features)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:21.04788Z","iopub.execute_input":"2023-03-21T17:58:21.048494Z","iopub.status.idle":"2023-03-21T17:58:21.056668Z","shell.execute_reply.started":"2023-03-21T17:58:21.048454Z","shell.execute_reply":"2023-03-21T17:58:21.055542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing the captions text","metadata":{}},{"cell_type":"code","source":"captions = captions[1:]\ncaptions[:5]","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:21.058474Z","iopub.execute_input":"2023-03-21T17:58:21.058822Z","iopub.status.idle":"2023-03-21T17:58:21.06892Z","shell.execute_reply.started":"2023-03-21T17:58:21.058795Z","shell.execute_reply":"2023-03-21T17:58:21.067794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"captions[8].split(',')[1]","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:21.070627Z","iopub.execute_input":"2023-03-21T17:58:21.070978Z","iopub.status.idle":"2023-03-21T17:58:21.081701Z","shell.execute_reply.started":"2023-03-21T17:58:21.070943Z","shell.execute_reply":"2023-03-21T17:58:21.080583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"captions_dict = {}\n\nfor cap in captions:\n    try:\n        img_name = cap.split(',')[0]\n        caption = cap.split(',')[1]\n        # Each image has 5 captions\n        if img_name in img_features:\n            if img_name not in captions_dict:\n                captions_dict[img_name] = [caption] # Storing the first caption\n            else:\n                captions_dict[img_name].append(caption) # Adding the remaining captions \n    except:\n        break","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:21.083318Z","iopub.execute_input":"2023-03-21T17:58:21.083858Z","iopub.status.idle":"2023-03-21T17:58:21.132194Z","shell.execute_reply.started":"2023-03-21T17:58:21.08382Z","shell.execute_reply":"2023-03-21T17:58:21.131249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(captions_dict)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:21.133559Z","iopub.execute_input":"2023-03-21T17:58:21.134006Z","iopub.status.idle":"2023-03-21T17:58:21.140685Z","shell.execute_reply.started":"2023-03-21T17:58:21.133967Z","shell.execute_reply":"2023-03-21T17:58:21.139605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_preprocess(text):\n    modified_text = text.lower() # Converting text to lowercase\n    modified_text = 'startofseq ' + modified_text + ' endofseq' # Appending the special tokens at the beginning and ending of text\n    return modified_text","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:21.142171Z","iopub.execute_input":"2023-03-21T17:58:21.142838Z","iopub.status.idle":"2023-03-21T17:58:21.150519Z","shell.execute_reply.started":"2023-03-21T17:58:21.142802Z","shell.execute_reply":"2023-03-21T17:58:21.14957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Storing the preprocessed text within the captions dictionary\nfor key, val in captions_dict.items():\n    for item in val:\n        captions_dict[key][val.index(item)] = text_preprocess(item)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:21.152122Z","iopub.execute_input":"2023-03-21T17:58:21.152484Z","iopub.status.idle":"2023-03-21T17:58:21.164577Z","shell.execute_reply.started":"2023-03-21T17:58:21.152449Z","shell.execute_reply":"2023-03-21T17:58:21.163832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating vocabulary of the entire text corpus","metadata":{}},{"cell_type":"code","source":"count_words = dict()\ncnt = 1\n\nfor key, val in captions_dict.items(): # Iterating through all images with keys as images and their values as 5 captions\n    for item in val: # Iterating through all captions for each image\n        for word in item.split(): # Iterating through all words in each caption\n            if word not in count_words:\n                count_words[word] = cnt\n                cnt += 1","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:21.166126Z","iopub.execute_input":"2023-03-21T17:58:21.166829Z","iopub.status.idle":"2023-03-21T17:58:21.193434Z","shell.execute_reply.started":"2023-03-21T17:58:21.166789Z","shell.execute_reply":"2023-03-21T17:58:21.192388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(count_words) # Vocab size","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:21.195192Z","iopub.execute_input":"2023-03-21T17:58:21.195973Z","iopub.status.idle":"2023-03-21T17:58:21.207986Z","shell.execute_reply.started":"2023-03-21T17:58:21.195936Z","shell.execute_reply":"2023-03-21T17:58:21.206842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding the text by assigning each word to its corresponding index in the vocabulary i.e. count_words dictionary\nfor key, val in captions_dict.items():\n    for caption in val:\n        encoded = []\n        for word in caption.split():\n            encoded.append(count_words[word])\n        captions_dict[key][val.index(caption)] = encoded","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:21.209652Z","iopub.execute_input":"2023-03-21T17:58:21.210125Z","iopub.status.idle":"2023-03-21T17:58:21.248409Z","shell.execute_reply.started":"2023-03-21T17:58:21.210091Z","shell.execute_reply":"2023-03-21T17:58:21.247575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Determining the maximum possible length of text within the entire captions text corpus\nmax_len = -1\n\nfor key, value in captions_dict.items():\n    for caption in value:\n        if max_len < len(caption):\n            max_len = len(caption)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:21.250295Z","iopub.execute_input":"2023-03-21T17:58:21.250882Z","iopub.status.idle":"2023-03-21T17:58:21.258299Z","shell.execute_reply.started":"2023-03-21T17:58:21.250848Z","shell.execute_reply":"2023-03-21T17:58:21.257355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:21.26102Z","iopub.execute_input":"2023-03-21T17:58:21.261343Z","iopub.status.idle":"2023-03-21T17:58:21.269939Z","shell.execute_reply.started":"2023-03-21T17:58:21.261314Z","shell.execute_reply":"2023-03-21T17:58:21.268773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(count_words) # Vocab size is the total number of words present in count_words dictionary\nvocab_size","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:21.271817Z","iopub.execute_input":"2023-03-21T17:58:21.272227Z","iopub.status.idle":"2023-03-21T17:58:21.279829Z","shell.execute_reply.started":"2023-03-21T17:58:21.272193Z","shell.execute_reply":"2023-03-21T17:58:21.278507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building a custom generator function to generate input image features, previously generated text and the text to be generated as output","metadata":{}},{"cell_type":"code","source":"def generator(img,caption):\n    n_samples = 0\n    X = []\n    y_input = []\n    y_output = []\n    \n    for key, val in caption.items(): \n        for item in val: \n            for i in range(1,len(item)):\n                X.append(img[key]) # Appending the input image features\n                input_seq = [item[:i]] # Previously generated text to be used as input to predict the next word \n                output_seq = item[i] # The next word to be predicted as output\n                # Padding encoded text sequences to the maximum length\n                input_seq = pad_sequences(input_seq,maxlen=max_len,padding='post',truncating='post')[0] \n                # One Hot encoding the output sequence with vocabulary size as the total no. of classes\n                output_seq = to_categorical([output_seq],num_classes=vocab_size+1)[0]\n                y_input.append(input_seq)\n                y_output.append(output_seq)\n    \n    return X, y_input, y_output","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:21.281428Z","iopub.execute_input":"2023-03-21T17:58:21.281809Z","iopub.status.idle":"2023-03-21T17:58:21.291973Z","shell.execute_reply.started":"2023-03-21T17:58:21.281774Z","shell.execute_reply":"2023-03-21T17:58:21.291003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y_in, y_out = generator(img_features,captions_dict)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:21.293644Z","iopub.execute_input":"2023-03-21T17:58:21.294035Z","iopub.status.idle":"2023-03-21T17:58:24.39549Z","shell.execute_reply.started":"2023-03-21T17:58:21.294Z","shell.execute_reply":"2023-03-21T17:58:24.394418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X), len(y_in), len(y_out)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:24.399409Z","iopub.execute_input":"2023-03-21T17:58:24.399725Z","iopub.status.idle":"2023-03-21T17:58:24.410148Z","shell.execute_reply.started":"2023-03-21T17:58:24.399696Z","shell.execute_reply":"2023-03-21T17:58:24.409093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting input and output into Numpy arrays for faster processing\nX = np.array(X)\ny_in = np.array(y_in,dtype='float64')\ny_out = np.array(y_out,dtype='float64')","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:24.412415Z","iopub.execute_input":"2023-03-21T17:58:24.413478Z","iopub.status.idle":"2023-03-21T17:58:25.748092Z","shell.execute_reply.started":"2023-03-21T17:58:24.413441Z","shell.execute_reply":"2023-03-21T17:58:25.746917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape, y_in.shape, y_out.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:25.750387Z","iopub.execute_input":"2023-03-21T17:58:25.750788Z","iopub.status.idle":"2023-03-21T17:58:25.760431Z","shell.execute_reply.started":"2023-03-21T17:58:25.750742Z","shell.execute_reply":"2023-03-21T17:58:25.759393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Establishing the model architecture","metadata":{}},{"cell_type":"code","source":"embedding_len = 128\nMAX_LEN = max_len\nvocab_size = len(count_words)\n\n# Model for image feature extraction\nimg_model = Sequential()\nimg_model.add(Dense(embedding_len,input_shape=(2048,),activation='relu'))\nimg_model.add(RepeatVector(MAX_LEN))\n\nimg_model.summary()\n\n# Model for generating captions from image features\ncaptions_model = Sequential()\ncaptions_model.add(Embedding(input_dim=vocab_size+1,output_dim=embedding_len,input_length=MAX_LEN))\ncaptions_model.add(LSTM(256,return_sequences=True))\ncaptions_model.add(TimeDistributed(Dense(embedding_len)))\n\ncaptions_model.summary()\n\n# Concatenating the outputs of image and caption models\nconcat_output = Concatenate()([img_model.output,captions_model.output])\n# First LSTM Layer\noutput = LSTM(units=128,return_sequences=True)(concat_output)\n# Second LSTM Layer\noutput = LSTM(units=512,return_sequences=False)(output)\n# Output Layer \noutput = Dense(units=vocab_size+1,activation='softmax')(output)\n# Creating the final model\nfinal_model = Model(inputs=[img_model.input,captions_model.input],outputs=output)\nfinal_model.compile(loss='categorical_crossentropy',optimizer='RMSprop',metrics='accuracy')\nfinal_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:25.767458Z","iopub.execute_input":"2023-03-21T17:58:25.768184Z","iopub.status.idle":"2023-03-21T17:58:27.286866Z","shell.execute_reply.started":"2023-03-21T17:58:25.768145Z","shell.execute_reply":"2023-03-21T17:58:27.285969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing the model architecture","metadata":{}},{"cell_type":"code","source":"plot_model(final_model,'model.png',show_shapes=True,dpi=100)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:27.293035Z","iopub.execute_input":"2023-03-21T17:58:27.299873Z","iopub.status.idle":"2023-03-21T17:58:27.779728Z","shell.execute_reply.started":"2023-03-21T17:58:27.29983Z","shell.execute_reply":"2023-03-21T17:58:27.778298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model training","metadata":{}},{"cell_type":"code","source":"mc = ModelCheckpoint('image_caption_generator.h5',monitor='accuracy',verbose=1,mode='max',save_best_only=True)\n\nfinal_model.fit([X,y_in],\n                y_out,\n                batch_size=512,\n                callbacks=mc,\n                epochs=200)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T17:58:27.782051Z","iopub.execute_input":"2023-03-21T17:58:27.78245Z","iopub.status.idle":"2023-03-21T18:49:34.734604Z","shell.execute_reply.started":"2023-03-21T17:58:27.782408Z","shell.execute_reply":"2023-03-21T18:49:34.733691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating an inverse dictionary with reverse key-value pairs\ninverse_dict = {val: key for key,val in count_words.items()}","metadata":{"execution":{"iopub.status.busy":"2023-03-21T18:49:34.736731Z","iopub.execute_input":"2023-03-21T18:49:34.737288Z","iopub.status.idle":"2023-03-21T18:49:34.744186Z","shell.execute_reply.started":"2023-03-21T18:49:34.737257Z","shell.execute_reply":"2023-03-21T18:49:34.743163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving the final trained model and the vocabulary dictionary","metadata":{}},{"cell_type":"code","source":"final_model.save('image_caption_generator.h5')","metadata":{"execution":{"iopub.status.busy":"2023-03-21T18:49:34.74571Z","iopub.execute_input":"2023-03-21T18:49:34.74607Z","iopub.status.idle":"2023-03-21T18:49:34.861616Z","shell.execute_reply.started":"2023-03-21T18:49:34.746027Z","shell.execute_reply":"2023-03-21T18:49:34.860591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('vocab.npy',count_words)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T18:49:34.863126Z","iopub.execute_input":"2023-03-21T18:49:34.863497Z","iopub.status.idle":"2023-03-21T18:49:34.870231Z","shell.execute_reply.started":"2023-03-21T18:49:34.863462Z","shell.execute_reply":"2023-03-21T18:49:34.869102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating sample predictions","metadata":{}},{"cell_type":"code","source":"# Custom function for extracting an image and transforming it into an appropriate format\ndef getImage(idx):\n    test_img_path = images[idx]\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img,cv2.COLOR_BGR2RGB)\n    test_img = cv2.resize(test_img,(224,224))\n    test_img = np.reshape(test_img,(1,224,224,3))\n    return test_img","metadata":{"execution":{"iopub.status.busy":"2023-03-21T18:49:34.871713Z","iopub.execute_input":"2023-03-21T18:49:34.872245Z","iopub.status.idle":"2023-03-21T18:49:34.880617Z","shell.execute_reply.started":"2023-03-21T18:49:34.872208Z","shell.execute_reply":"2023-03-21T18:49:34.879542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    random_no = np.random.randint(0,1501,(1,1))[0,0]\n    test_feature = model.predict(getImage(random_no)).reshape(1,2048)\n    test_img_path = images[random_no]\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img,cv2.COLOR_BGR2RGB)\n    pred_text = ['startofseq']\n    count = 0\n    caption = '' # Stores the predicted captions text\n    \n    while count < 25:\n        count += 1\n        # Encoding the captions text with numbers\n        encoded = []\n        \n        for i in pred_text:\n            encoded.append(count_words[i])\n        \n        encoded = [encoded]\n        # Padding the encoded text sequences to maximum length\n        encoded = pad_sequences(encoded,maxlen=MAX_LEN,padding='post',truncating='post')\n        pred_idx = np.argmax(final_model.predict([test_feature,encoded])) # Fetching the predicted word index having the maximum probability of occurrence\n        sampled_word = inverse_dict[pred_idx] # Extracting the predicted word by its respective index\n        # Checking for ending of the sequence\n        if sampled_word == 'endofseq':\n            break\n        caption = caption + ' ' + sampled_word\n        pred_text.append(sampled_word)\n    \n    plt.figure(figsize=(5,5))\n    plt.imshow(test_img)\n    plt.xlabel(caption)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T18:51:43.302943Z","iopub.execute_input":"2023-03-21T18:51:43.303864Z","iopub.status.idle":"2023-03-21T18:51:54.771607Z","shell.execute_reply.started":"2023-03-21T18:51:43.303805Z","shell.execute_reply":"2023-03-21T18:51:54.770721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow, simply astonishing! The image caption generation model has produced highly accurate predictions for all the above random images. <br>\n\nSo, this is it from my side. Kindly upvote my work if you absolutely loved it!","metadata":{}}]}